---
title: "Weekly Summary 12"
author: "Alvaro Tapia"
title-block-banner: true
title-block-style: default
toc: true
#format: html
format: pdf
---

---


## Tuesday, April 11

::: {.callout-important}
## TIL

In this weekday, we didn't make any recap, we moved forward to reading the datasets that we were going to use for today's class. We first, learned how to change variables into integers for later usage. We then learned how to use benchmark for logistic regression. We created a new Neural Net Model nad we fitted it using Luz. We finally talked about DataLoaders and a little bit amout Image Classication.

Today we learned the following concepts:

1. Learned how to change the variables of a column in a dataset to binary integers.

1. Understood how to use Benchmark in Logistic Regression

1. Knew how to create a Neural Net Model

1. Learned how to fit the model using Luz Package

1. Understood what are DataLoaders and how they work

1. Finally we covered very briefly Image Classification

:::

Libraries used for this week:

```{R results='hide'}
#Libraries to be used
library(ggplot2)
library(knitr)
library(magrittr)
library(readr)
library(tidyr)
library(ggplot2)
library(dplyr)
library(purrr)
library(cowplot)
library(ISLR2)
library(torch)
library(caret)
library(glmnet)
library(nnet)
library(rpart)
library(rpart.plot)
library(e1071)
library(luz)
library(torchvision)
```

1. We started by learning about how to change the variables of the parameters of the titanic dataset and the breast cancer dataset. For the purpose of this summary I will be only running the brast cancer dataset to run all the other code.

Titanic Dataset

```{R}
# url <- "https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv"

# df <- read_csv(url) %>%
#     mutate_if(\(x) is.character(x), asfactor) %>%
#     mutate(y = Survived) %>%
#     select(-c(Name, Survived)) %>%
#     (\(x) {
#       names(x) <- tolower(names(x))
#       x
#     })
# df %>% head
```

Breast cancer dataset

```{R}
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"

col_names <- c("id", "diagnosis", paste0("feat", 1:30))

df <- read_csv(
        url, col_names, col_types = cols()
    ) %>%
    select(-id) %>%
    mutate(y = ifelse(diagnosis == "M", 1, 0)) %>%
    select(-diagnosis)

df %>% head
```

After that, we understood how to train and test split the dataset using k as 5.

Train/test split

```{R}
k <- 5

test_ind <- sample(
  1:nrow(df),
  floor(nrow(df) / k),
  replace=FALSE
)
```

```{R}
df_train <- df[-test_ind, ]
df_test  <- df[test_ind, ]

nrow(df_train) + nrow(df_test) == nrow(df)
```

2. We then learned about using Benchmark with Logistic Regression, which is basically using the Logistic Regression algorithm as a baseline or reference model to compare the performance of other machine learning models or algorithms. We can represent this in the following code:

```{R}
fit_glm <- glm(
    y ~ ., 
    df_train %>% mutate_at("y", factor), 
    family = binomial()
)

glm_test <- predict(
    fit_glm, 
    df_test,
    output = "response"
)

glm_preds <- ifelse(glm_test > 0.5, 1, 0)
table(glm_preds, df_test$y)
```

3. After that, we learned the main topic of this class, which is creating a Neural Net Model. This is known as a simplified model of the way the human brain processes information. We understood that it works by simulating a large number of interconnected processing units that resemble abstract versions of neurons.

```{R}
NNet <- nn_module(
  initialize = function(p, q1, q2, q3) {
    self$hidden1 <- nn_linear(p, q1)
    self$hidden2 <- nn_linear(q1, q2)
    self$hidden3 <- nn_linear(q2, q3)
    self$output <- nn_linear(q3, 1)
    self$activation <- nn_relu()
    self$sigmoid <- nn_sigmoid()
  },

  forward = function(x) {
    x %>%
      self$hidden1() %>% self$activation() %>%
      self$hidden2() %>% self$activation() %>%
      self$hidden3() %>% self$activation() %>%
      self$output() %>% self$sigmoid()
  }
)
```

```{R}
NNet(p=2,q1=2,q2=2,q3=10)
```

4. We then learned how to fit this model using luz and we also understood how to change the variables of a factor as binary variables. We only use this code for the Titanic Dataset.

```{R}
#Only to be used when having to change the variables of a factor as binary variables
#Used when working with the Titanic Dataset
M <- model.matrix(y ~ 0 + ., data = df_train)
```

```{R}
# fit_nn <- NNet %>%
# #Setting the model
#     setup(
#       loss = nn_bce_loss(),
#       optimizer = optim_rmsprop,
#       metrics = list(
#         luz_metric_accuracy()
#       )
#     ) %>%
# #Setting the hyperparameters
#     set_hparams(p=150,q1=50, q2=30, q3=20) %>%
#     set_opt_hparams(lr=0.01) %>%
# #Fitting the model
#     fit(
#       data=list(
#         df_train %>%
#           select(-y) %>%
#           as.matrix,
#         df_train %>%
#           select(y) %>%
#           as.matrix
#       ),
#       valid_data=list(
#         df_train %>%
#           select(-y) %>%
#           as.matrix,
#         df_train %>%
#           select(y) %>%
#           as.matrix
#       ),
#       epochs=50,
#       verbose=TRUE
#     )

fit_nn <- NNet %>%
    #
    # Setup the model
    #
    setup(
        loss = nn_bce_loss(),
        optimizer = optim_adam, 
        metrics = list(
            luz_metric_accuracy()
        )
    ) %>% 
    #
    # Set the hyperparameters
    #
    set_hparams(p=ncol(M), q1=256, q2=128, q3=64) %>% 
    set_opt_hparams(lr=0.005) %>% 
    #
    # Fit the model
    #
    fit(
        data = list(
            model.matrix(y ~ 0 + ., data = df_train),
            df_train %>% select(y) %>% as.matrix
        ),
        valid_data = list(
            model.matrix(y ~ 0 + ., data = df_test),
            df_test %>% select(y) %>% as.matrix
        ),
        epochs = 50, 
        verbose = TRUE
    )
```

Plotting the results

```{R}
plot(fit_nn)
```

After that, we learned how to make predictinos using these plots

```{R}
nn_test <- predict(
    fit_nn, 
    model.matrix(y ~ . - 1, data = df_test)
)
nn_preds <- ifelse(nn_test > 0.5, 1, 0)

table(nn_preds, df_test$y)
```

5. Then, we learned about DataLoaders, these are a key component in the machine learning pipeline. Their main purpose is to handle loading and preprocessing data in a way that is efficient for training and evaluating models. They make it easier to work with large datasets in smaller chunks.

We learned that they are very efficient and people should use it because they have:

-Efficient memory management by reducing memory usage
-Parallelism by supporting asynchronous data loading for faster processing
-Preprocessing by applying data transformations during training and evaluation
-Flexibility by easily switching between different datasets
-Standarization by having consistent data formats accross various ML projects

```{R}
transform <- function(x) x %>% 
    torch_tensor() %>% 
    torch_flatten() %>% 
    torch_div(255)
```

```{R}
dir <- "./mnist"

train_ds <- mnist_dataset(
    root = dir,
    train = TRUE,
    download = TRUE,
    transform = transform
)
test_ds <- mnist_dataset(
    root = dir,
    train = FALSE,
    download = TRUE,
    transform = transform
)
```

Identify an imagine and creating it using dataloaders

```{R}
options(repr.plot.width=10, repr.plot.height=10)

i <- sample(1:length(train_ds), 1)
x <- train_ds$data[i, ,] %>% t

image(x[1:28, 28:1], useRaster=TRUE, axes=FALSE, col=gray.colors(1000), main = train_ds$targets[i]-1 )
```

6. We finally talked a little bit about Image Classification an how to use machine learning to classify an image of a number into an actual number.


## Thursday, April 13

::: {.callout-important}
## TIL

In this weekday we learned a lot of things. First, we talked about very briefly about Homework 5 and also about binary classification to learn its main goal. After that, we moved forward to the main topic which was Image Classification where we talked about it more in depth in this clase and we learned what it is and its main purpose. Along with that we understood how to generate predictions with that and also about using soft-max. We then talked about Unsupervised learning and we understood how important they are, when we should use it, and more. Finally, we studied more in depth about using unsupervised learning when using Principal Component Analysis in order to produce a low dimensional representation for a dataset.

1. Talked about the Hw5 and understood binary classification as well

1. Learned about Image Classification entirely

1. Learned why, how, and when, Unsupervised learning is important

1. Undersood how to use Principal Component Analysis

:::

::: {.callout-warning}

The code that is commented out is code that couldn't be ran because the professor didn't provide the dataframe

:::

1. We first started the class by talking a little bit about binary classification, where we understood that its main goal is to predict one of two possible outcomes for a given input, where it basically categorizes new observations into one of two classes.

2. We then continued studying what we learned at the end of the class on Tuesday which is predicting what the digits of the images presented are. This concept is called Image Classification, which technically it is a supervised learning problem: define a set of target classes (objects to identify in images), and train a model to recognize them using labeled example photos. In addition to that, we understood that we can use optimization for the creation of loops. We used the following code as an example of what we learned:

```{R}
train_dl <- dataloader(train_ds, batch_size = 1024, shuffle = TRUE)
test_dl <- dataloader(test_ds, batch_size = 1024)
```

We learned how to generate the code to make the predictions

```{R}

# Defining the Nerual Net Model for Image Classication
NNet_10 <- nn_module(
  initialize = function(p, q1, q2, q3, o) {
    self$hidden1 <- nn_linear(p, q1)
    self$hidden2 <- nn_linear(q1, q2)
    self$hidden3 <- nn_linear(q2, q3)
    self$OUTPUT <- nn_linear(q3, o)
    self$activation <- nn_relu()
  },
  forward = function(x) {
    x %>%
      self$hidden1() %>%
      self$activation() %>%
      self$hidden2() %>%
      self$activation() %>%
      self$hidden3() %>%
      self$activation() %>%
      self$OUTPUT()
  }
)
```

```{R}
# Fitting the model
fit_nn <- NNet_10 %>%
    #
    # Setup the model
    #
    setup(
        loss = nn_cross_entropy_loss(),
        optimizer = optim_adam,
        metrics = list(
            luz_metric_accuracy()
        )
    ) %>%
    #
    # Set the hyperparameters
    #
    set_hparams(p=28*28, q1=256, q2=128, q3=64, o=10) %>% 
    #
    # Fit the model
    #
    fit(
        epochs = 10,
        data = train_dl,
        # valid_data = test_dl,
        verbose=TRUE
    )

NN10_preds <- fit_nn %>% 
  predict(test_ds) %>% 
  torch_argmax(dim = 2) %>%
  as_array()
```

We then learned how to use the soft-max function in order to assign decimal probabilities to each class for this multi-class problem.

```{R}
# probs <- 
# probs
# The code created was supposed to be uploaded by the professor but he didn't
```

After that we learned how confusion matrix works, we used as an example the confusion that the algorithm could have when trying to classify the numbers 4 and 9 because they can be very alike sometimes. 

3. As a second topic, we learned and talked about Unsupervised learning which is something new to us since we have been working mostly with supervised learning methods. We call something unsupervised learning when we don't have acces to the labeled data. In this sense, our main goal would be to understand the relationship between dimension reduction and clustering. We also learned why we would want to use unsupervised learning over supervised learning and usually the main reason is because it will always be easier to obtain unlabeled data as opposed to labaled data. 


4.For this class we focused on using Principal Component Analysis which in charge of producing a low dimensional representation for a dataset. The first step for this analysis was to combine normalize the principal component or first component of z1 through a combination of features, and then you keep doing that with later steps. We also understood that what we want to do, is to find a direction that maximizes the values. The code created for this part was supposed to be uploaded before the weekly summary due date but the professor didn't, but this explains almost everything done.
