---
title: "Weekly Summary 9"
author: "Alvaro Tapia"
title-block-banner: true
title-block-style: default
toc: true
#format: html
format: pdf
---

---


## Tuesday, March 21

::: {.callout-important}
## TIL

In this weekday, we finished to wrap up what we learned on week 8, and we proceeded to continue our agenda by learning about decision boundary and what it is, then we continued to learned about classification accuracy and confusion matrix where we did some coding to represent exactly how they look like and to understand ther main purpose. And with that we concluded the class. It is important to note that at the end we talked a little bit about Multinomial Logistic regression but we mainly touch that topic on the next class.

Today we learned the following concepts:

1. Decision boundary 

1. Classification accuracy

1. Confusion matrix

:::

Libraries used for this week:

```{R results='hide'}
#Libraries to be used
library(ggplot2)
library(knitr)
library(magrittr)
library(readr)
library(tidyr)
library(ggplot2)
library(dplyr)
library(purrr)
library(cowplot)
library(ISLR2)
library(torch)
library(caret)
library(glmnet)
library(nnet)
```

1. We started by learning about Decision Boundary, this is basically the region of a problem space in which the output label of a classifier is ambiguous. In the following code we can appreciate just the values that are going to be analysed later to create the decision boundary:

```{R}
library(class)
X <- t(replicate(100, runif(2)))
y <- ifelse(apply(X, 1, \(x)sum(x^1.3)) + 0.1 * rnorm(500) <= 1, 0, 1) %>% as.factor()
col <- ifelse(y == 0, "blue", "red")

plot(X[,1], X[,2], col=col)
```

```{R}
df <- data.frame(y=y, x1=X[,1], x2=X[,2])
model <- glm(y ~ ., df, family=binomial())
summary(model)
```

2. Later, we learned about Classification accuracy which is the ratio of number of correct predictions to the total number of input samples.

Code used to better understand the topic:

```{R}
xnew <- data.frame(
  x1 = rep(seq(0, 1, length.out=50), 50),
  x2 = rep(seq(0, 1, length.out=50), each = 50)
)

prob <- predict(model, xnew, type="response")
decision <- ifelse(prob < 0.5, "blue", "red")

plot(xnew[,1], xnew[,2], col=decision, pch=22)
points(X[,1], X[,2], col=col)
```

3. Then, we learned about Confusion Matrix which presents a table layout of the different outcomes of the prediction and results of a classification problem and helps visualize its outcomes:

```{R}
idx <- sample(1:nrow(df), 50)
train <- df[-idx, ]
test <- df[idx, ]

model <- glm(y~., train, family=binomial())
probs <- predict(model, test, type="response")
```

```{R}
predicted <- ifelse(probs < 0.5, 0, 1)
expected <- test$y

table(predicted, expected)
```

This also helps us notice if there are any false postivies and viceversa. 

```{R}
caret::confusionMatrix(data=as.factor(predicted), reference = as.factor(expected))
```

This shows the accuracy, the p-value, and CI as the most important ones. And with that, it is the end of logistic regression where we learned how to train models.


## Thursday, March 23

::: {.callout-important}
## TIL

In this weekday we learned a lot of things. First, we started the class with identifying the differences between logistic regression and multinomial logistic regression focusing more on the last one. Then, we learned about the grid of decision boundary. After that me moved on by learining about decision trees and their classification where we also learned how to code them. Then, we understood a subtopic called suport vector machine to finally learn about neural networks but we will focus on that more next week.

Today, I learnt the following concepts in class:

1. Multinomial logistic regression

1. Define the grid of decision boundary

1. Classification (decision) tree

1. Suport Vector Machine

1. Generalize using a neural network with one hidden layer
:::

1. First, we learned about the differences of logistic regression and multinomial logistic regression. For example, when you want to interprete the relative risk of deaseases or not deseases the goal is to conclude some relationship between the results. It should be noted that we learned that Multinomial Logistic regression is 

Below I'll use some code implemented on class as an example 1.3 in order to show the topics learned described above:

```{R}
sample(1:3, size = 1000, replace = TRUE, prob = c(0.8,0.1,0.1))
```

```{R}
b <- c(-5, 0 ,5)
prob_function = \(x) exp(b*x) / sum(exp(b*x))
x <- rnorm(100)
y <- c()
for (i in 1:length(x)){
  y[i] <- sample(0:2, 1, prob = prob_function(x[i]))
}
cbind(x, y) %>% head
```

Then we learned how to choose the category as 1:

```{R}
df <- data.frame(x=x, y=as.factor(y))
df$y <- relevel(df$y, ref="1")
df$y
```

2. After that, we learned how to predict a class using logistic regression model, and we also learned about defining the grid of decision boundary. This can be represented in the following coding.

This predicts what class is gonna be using logistic regression model:

```{R}
n <- 250
X <- t(replicate(n, 2 *runif(2) - 1))
y <- ifelse(apply(X, 1, \(x) sum(sign(x + 0.01 * rnorm(2)))) != 0, 0, 1) %>% as.factor()

col <- ifelse(y == 0, "blue", "red")
plot(X[,1], X[,2], col=col, pch=19)
```

```{R}
df <- data.frame(y=y, x1=X[, 1], x2=X[, 2])
model <- glm(y ~ x1 + x2, df, family=binomial())
f_logistic = \(x) predict(model, data.frame(x1=x[,1], x2=x[,2]), type="response")
```

Defining the grid of decision boundary

```{R}
xnew <- cbind(
  rep(seq(-1.1, 1.1, length.out=50), 50),
  rep(seq(-1.1, 1.1, length.out=50), each = 50)
)
```

Plotting the decision boundary

```{R}
plt <- function(f, x){
  plot(x[, 1], x[, 2], col=ifelse(f(x) < 0.5, "blue", "red"), pch=22)
  points(df$x1, df$x2, col=ifelse(y == "0", "blue", "red"), pch=19)
}

overview <- function(f){
  predicted <- ifelse(f(df[, -1]) < 0.5, 0, 1)
  actual <- df[, 1]
  table(predicted, actual)
}
```

This displays the decision boundary with logistric regression model but we learned that the following grid doesnt make sense since we are supposed to have blue on top right and bottom left so the main goal is to find a method to have the right selected data.

```{R}
plt(f_logistic, xnew)
```


3. Later on, we learned about Classification (decision) tree. This is commonly used in machine learning to predict catergorical outcomes. They are hierarchical model that recursively partions the data into smaller datasets based on the most information features, and each partion is associated with a class label.

With this, we also learned how to use the packages rpart.plot and rpart in order to create decision tress:

```{R}
library(rpart)
library(rpart.plot)

dtree <- rpart(y ~ x1 + x2, df, method="class")
rpart.plot(dtree)
```

Then we predicted the values for the decision tree to later create the actual decision tree.
```{R}
predict(dtree, data.frame(x1=xnew[,1], x2=xnew[,2]), type="class")
```


4. Finally we learned about using Support vector machine, which are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. 

```{R}
#This wil display something similar to a diamond
n <- 750
X <- t(replicate(n, 2 * runif(2) - 1))
y <- ifelse(apply(X, 1, \(x) sum(abs(x))) + 0.1 * rnorm(n) <= 1, 0, 1) %>% as.factor()
col <- ifelse(y == 0, "blue", "red")
df <- data.frame(y=y, x1=X[, 1], x2=X[, 2])

plot(X[,1], X[,2], col=col, pch=19)
```

With this, first we tried using logistic regression to separately correctly but it didn't work, so we proceeded to use classification tree and learn how create them:

```{R}
dtree <- rpart(y ~ x1 + x2, df, method="class")
rpart.plot(dtree)

f_dtree <- \(x) as.numeric(predict(dtree, data.frame(x1=x[,1], x2=x[,2]), type = "class")) - 1
plt(f_dtree, xnew)
```

Understanding better support vector machine:

```{R}
library(e1071)
```

```{R}
svm_model <- svm(y ~ x1 + x2, df, kernel="sigmoid")
summary(svm_model)
```

```{R}
f_svm <- \(x) predict(svm_model, x) %>% as.numeric() - 1
plt(f_svm, xnew)
```

5. At the end of the class, we talked a little bit about neural networks, which is a method in artificial intelligence that teaches computers to process data in a way that is inspired by the human brain.

This is the code we developed to better understand this concept:

```{R}
module <- nn_module(
  initialize = function() {
    self$g <- nn_linear(2, 1)
    self$h <- nn_sigmoid()
  },
  forward = function(x) {
    x %>%
      self$f() %>%
      self$g() %>%
      self$h()
  }
)
```
